# ğŸ–¼ï¸ Image Caption Generator

This project is a **Deep Learningâ€“based Image Captioning System** built using the **BLIP (Salesforce/blip-image-captioning-base)** model. It generates a caption for any given input image.

The project includes:

* Backend code (model loading + caption generation)
* A clean **Gradio interface** for user interaction
* Sample output imagesÂ 
* Stepâ€‘byâ€‘step setup instructions for **Kaggle** and **local machines**

---

## ğŸš€ Features

* Generates captions using a pretrained BLIP model
* Supports both **image upload** and **image URL input**
* Works on **Kaggle GPU (T4)** or any system with PyTorch
* Simple Gradio UI
* Easy to modify and extend

---

## ğŸ“‚ Project Structure

Image-Caption-Generator/
â”‚
â”œâ”€â”€ backend_captioning_code.py       # Backend model + caption generation code
â”œâ”€â”€ gradio_interface.py              # Gradio UI for captioning
â”œâ”€â”€ ImageCaption.ipynb               # Jupyter Notebook version
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”‚
â”œâ”€â”€ output_samples/                  # Folder containing sample output images
â”‚   â”œâ”€â”€ 1.PNG
â”‚   â”œâ”€â”€ 2.PNG
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ .gitattributes                   # Git settings
â””â”€â”€ README.md                        # Project documentation

---

## ğŸ› ï¸ Requirements

Create a **requirements.txt** file with the following:

```
transformers
torch
torchvision
pillow
gradi o
requests
```

---

## ğŸ“¥ Running the Project on Kaggle

Follow these steps to run the project successfully.

### **1ï¸âƒ£ Open Kaggle Notebook**

* Go to **Kaggle â†’ Notebooks**
* Create a **New Notebook**
* Enable **Accelerator â†’ GPU (T4)**

### **2ï¸âƒ£ Upload Your Input Image**

You can add your own image:

* Left sidebar â†’ **Add Data** â†’ **Upload File** â†’ Choose your image
* After upload, Kaggle creates a folder path like:

```
/kaggle/input/yourimagename/your_image.jpg
```

### **3ï¸âƒ£ Add Image Path in Backend Code**

In your backend code, change the following line:

```python
caption = predict_caption("/kaggle/input/yourfoldername/your_image.jpg")
```

Replace:

* `yourfoldername` â†’ the name of the uploaded dataset folder
* `your_image.jpg` â†’ your actual image file name

This is **mandatory** for the model to read your input.

### Example:

```python
caption = predict_caption("/kaggle/input/dogphoto/dog.png")
```

### **4ï¸âƒ£ Run the Backend Code**

The model loads BLIP and generates the caption.

---

## ğŸ–¥ï¸ Gradio Interface

To launch the interactive UI:

* Scroll to the Gradio block section
* Run the interface cell
* A public shareable link will appear

Users can:

* Upload an image
* Paste an image URL
* Get instant captions

---

## ğŸ§ª Sample Output Images

These are sample output images generated during testing.

### **ğŸ–¼ï¸ Output 1**

![Output 1](output_samples/1.PNG)





### **ğŸ–¼ï¸ Output 2**

![Output 2](output_samples/2.PNG)

---

## ğŸ§© How the Backend Works

### **1. Load Pretrained Model**

```python
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
```

### **2. Generate Caption**

```python
def predict_caption(image_path):
    image = Image.open(image_path)
    inputs = processor(image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)
```

### **3. Run on Your Image**

```python
caption = predict_caption("/kaggle/input/yourfolder/image.jpg")
print(caption)
```

---

## ğŸ“Œ Notes

* Doesnâ€™t require the dataset â€” only **one input image** is enough
* You just need to update the image path in the code
* Works offline after model download

---

## ğŸ“ Contact

Created by RakRoxÂ For improvements, open an issue or submit a pull request.

---

â­ *If you like this project, consider giving it a star on GitHub!*
