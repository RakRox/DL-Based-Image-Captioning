# ğŸ–¼ï¸ Image Caption Generator

A **Deep Learningâ€“based Image Captioning System** using the **BLIP (Salesforce/blip-image-captioning-base)** model. This project generates captions for any input image using a pretrained visionâ€“language transformer.

This repository includes:

* Backend caption generation code
* A Gradio-based user interface
* Sample output images
* Complete step-by-step guide for Kaggle and local setups

---

## ğŸš€ Features

* Caption generation using BLIP
* Accepts **image upload** and **image URL**
* Runs smoothly on **Kaggle GPU (T4)**
* Simple and clean Gradio UI
* Beginner-friendly structure

---

## ğŸ“‚ Project Structure

```
Image-Caption-Generator/
â”‚
â”œâ”€â”€ backend_captioning_code.py       # Backend model + caption generation code
â”œâ”€â”€ gradio_interface.py              # Gradio UI for captioning
â”œâ”€â”€ ImageCaption.ipynb               # Jupyter Notebook version
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”‚
â”œâ”€â”€ output_samples/                  # Folder containing sample output images
â”‚   â”œâ”€â”€ 1.PNG
â”‚   â”œâ”€â”€ 2.PNG
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ .gitattributes                   # Git settings
â””â”€â”€ README.md                        # Project documentation
```

---

## ğŸ› ï¸ Requirements

Your `requirements.txt` must include:

```
transformers
torch
torchvision
pillow
gradi o
requests
```

---

## ğŸ“¥ Running the Project on Kaggle

Follow this process to execute your captioning model.

### **1ï¸âƒ£ Create a Kaggle Notebook**

* Go to **Kaggle â†’ Notebooks**
* Click **New Notebook**
* Enable **GPU (T4)** under accelerator

---

### **2ï¸âƒ£ Upload Your Input Image**

* Left sidebar â†’ **Add Data** â†’ **Upload** â†’ select your image
* Kaggle assigns a path like:

```
/kaggle/input/yourfoldername/your_image.jpg
```

---

### **3ï¸âƒ£ Update Image Path in Code**

Inside `backend_captioning_code.py`, edit:

```python
caption = predict_caption("/kaggle/input/yourfoldername/your_image.jpg")
```

Replace:

* `yourfoldername` â†’ uploaded dataset folder
* `your_image.jpg` â†’ your actual filename

**Example:**

```python
caption = predict_caption("/kaggle/input/dogphoto/dog.png")
```

---

### **4ï¸âƒ£ Run the Backend Code**

The BLIP model loads and generates a caption.

---

## ğŸ–¥ï¸ Running the Gradio Interface

* Scroll to the Gradio section in the notebook
* Run the cell
* A shareable link will appear

You can:

* Upload an image
* Paste URL
* Get captions instantly

---

## ğŸ§ª Sample Output Images

Below are the provided test images.

### ğŸ–¼ï¸ Output 1

![Output 1](output_samples/1.PNG)

### ğŸ–¼ï¸ Output 2

![Output 2](output_samples/2.PNG)

---

## ğŸ§© How the Backend Works

### **1. Load Model**

```python
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
```

### **2. Define Caption Function**

```python
def predict_caption(image_path):
    image = Image.open(image_path)
    inputs = processor(image, return_tensors="pt")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)
```

### **3. Generate Caption**

```python
caption = predict_caption("/kaggle/input/yourfolder/image.jpg")
print(caption)
```

---

## ğŸ“Œ Notes

* Dataset not required â€” **one image is enough**
* Only update image path
* Works offline after model download

---

## ğŸ“ Contact

Created by **RakRox**.
Open an issue or submit a PR for improvements.

---

â­ **If you found this useful, please give it a star on GitHub!**
