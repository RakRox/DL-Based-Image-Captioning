{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['TRANSFORMERS_VERBOSITY'] = 'error'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport logging\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n!pip install transformers datasets torch torchvision pillow -q\n\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\nprint(\"ğŸ”„ Loading model...\")\n\n# Load model \nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\nprint(\"âœ… Model loaded successfully!\")\n\n# Function to generate caption\ndef predict_caption(image_path):\n    image = Image.open(image_path)\n    inputs = processor(image, return_tensors=\"pt\")\n    out = model.generate(**inputs)\n    return processor.decode(out[0], skip_special_tokens=True)\n\n# Test with your image\ncaption = predict_caption(\"/kaggle/input/flickr8k/Images/1001773457_577c3a7d70.jpg\")\nprint(\"ğŸ“ Caption:\", caption)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T12:59:02.175293Z","iopub.execute_input":"2025-11-20T12:59:02.175581Z","iopub.status.idle":"2025-11-20T13:00:53.613529Z","shell.execute_reply.started":"2025-11-20T12:59:02.175558Z","shell.execute_reply":"2025-11-20T13:00:53.612475Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-20 13:00:26.689845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763643627.042276      97 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763643627.173148      97 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"ğŸ”„ Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd128a1c15064cb987ae19a30561b996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c893df7e7db4bbb84b39d0cbd975dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a7fefa6fff40dc8a70a27d406f68fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9507aa7c3bad42598b0770544d759a41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1293f8126c214372b856876355afb3a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1568f2851be1464d93ecb4194aa0172f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc2476b0598426083841e95f2180393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def669b6409141f4a50089cd41cba184"}},"metadata":{}},{"name":"stdout","text":"âœ… Model loaded successfully!\nğŸ“ Caption: two dogs playing on the road\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ['TRANSFORMERS_VERBOSITY'] = 'error'\nos.environ['GRADIO_ANALYTICS_ENABLED'] = 'False'\n\nimport logging\nlogging.getLogger().setLevel(logging.ERROR)\n\n!pip install gradio transformers torch pillow -q > /dev/null 2>&1\n\nimport gradio as gr\nimport torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nprint(\"ğŸ”„ Setting up AI Caption Generator...\")\n\n# Load model quietly\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\nprint(\"âœ… AI Caption Generator is ready!\")\n\ndef generate_caption(image):\n    \"\"\"Generate caption for uploaded image\"\"\"\n    try:\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n        \n        inputs = processor(image, return_tensors=\"pt\")\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_length=50,\n                num_beams=5,\n                early_stopping=True\n            )\n        \n        caption = processor.decode(outputs[0], skip_special_tokens=True)\n        return caption\n    \n    except Exception as e:\n        return \"Sorry, I couldn't generate a caption for this image.\"\n\ndef generate_caption_from_url(url):\n    \"\"\"Generate caption from image URL\"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        image = Image.open(BytesIO(response.content))\n        return generate_caption(image)\n    except:\n        return \"Please check your internet connection or try a different image URL.\"\n\n# Create clean Gradio interface\nwith gr.Blocks(theme=gr.themes.Soft(), title=\"Image Caption Generator\") as demo:\n    \n    gr.Markdown(\n        \"\"\"\n        <div style=\"text-align: center;\">\n        <h1>ğŸ–¼ Image Caption Generator</h1>\n        <p>Upload any image and get an automatic caption instantly!</p>\n        </div>\n        \"\"\"\n    )\n    \n    with gr.Tab(\"ğŸ“¸ Upload Image\"):\n        with gr.Row():\n            with gr.Column():\n                gr.Markdown(\"### Step 1: Upload your image\")\n                image_input = gr.Image(\n                    label=\"\",\n                    type=\"pil\",\n                    height=300,\n                    show_label=False\n                )\n                upload_btn = gr.Button(\"âœ¨ Generate Caption\", variant=\"primary\", size=\"lg\")\n            \n            with gr.Column():\n                gr.Markdown(\"### Step 2: Your generated caption\")\n                caption_output = gr.Textbox(\n                    label=\"\",\n                    placeholder=\"Your image caption will appear here...\",\n                    lines=4,\n                    show_label=False\n                )\n        \n        gr.Markdown(\"Try these example images:\")\n        gr.Examples(\n            examples=[\n                [\"https://images.unsplash.com/photo-1541963463532-d68292c34b19?w=400\"],\n                [\"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=400\"],\n                [\"https://images.unsplash.com/photo-1575936123452-b67c3203c357?w=400\"]\n            ],\n            inputs=image_input,\n            outputs=caption_output,\n            fn=generate_caption,\n            cache_examples=True,\n            label=\"\"\n        )\n    \n    with gr.Tab(\"ğŸ”— From Web URL\"):\n        with gr.Row():\n            with gr.Column():\n                gr.Markdown(\"### Paste image URL from web\")\n                url_input = gr.Textbox(\n                    label=\"\",\n                    placeholder=\"Paste image link here...\",\n                    lines=2,\n                    show_label=False\n                )\n                url_btn = gr.Button(\"ğŸŒ Generate Caption\", variant=\"primary\")\n            \n            with gr.Column():\n                url_output = gr.Textbox(\n                    label=\"\",\n                    placeholder=\"Caption from web image...\",\n                    lines=4,\n                    show_label=False\n                )\n    \n    # Footer\n    gr.Markdown(\n        \"\"\"\n        ---\n        <div style=\"text-align: center; color: #666;\">\n        <p>ğŸ’¡ <strong>Tips for better captions:</strong> Use clear, well-lit photos with recognizable objects and scenes.</p>\n        <p>Perfect for: Nature photos â€¢ People â€¢ Animals â€¢ Objects â€¢ Everyday scenes</p>\n        </div>\n        \"\"\"\n    )\n    \n    # Event handlers\n    upload_btn.click(\n        fn=generate_caption,\n        inputs=image_input,\n        outputs=caption_output\n    )\n    \n    url_btn.click(\n        fn=generate_caption_from_url,\n        inputs=url_input,\n        outputs=url_output\n    )\n\n# Launch quietly\nprint(\"ğŸš€ Starting Image Caption Generator...\")\ndemo.launch(share=True, quiet=True, show_error=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:02:37.661947Z","iopub.execute_input":"2025-11-20T13:02:37.663287Z","iopub.status.idle":"2025-11-20T13:03:09.070428Z","shell.execute_reply.started":"2025-11-20T13:02:37.663253Z","shell.execute_reply":"2025-11-20T13:03:09.069713Z"}},"outputs":[{"name":"stdout","text":"ğŸ”„ Setting up AI Caption Generator...\nâœ… AI Caption Generator is ready!\nğŸš€ Starting Image Caption Generator...\nCaching examples at: '/kaggle/working/.gradio/cached_examples/13'\n* Running on public URL: https://ddee70e62f5b436f34.gradio.live\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://ddee70e62f5b436f34.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}